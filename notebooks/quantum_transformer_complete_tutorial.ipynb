{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Quantum Transformer: Complete Tutorial and Experiments\n",
                "\n",
                "This notebook provides a comprehensive guide to the Quantum Transformer library, covering:\n",
                "\n",
                "1. **Installation and Setup** - Environment configuration and library imports\n",
                "2. **Basic Quantum Transformer Experiments** - Model creation and forward pass\n",
                "3. **Quantum Self-Attention** - Attention mechanisms and visualization\n",
                "4. **Molecular Property Prediction** - SMILES tokenization and energy prediction\n",
                "5. **Benchmarking and Analysis** - Performance comparison with classical models\n",
                "6. **Advanced Quantum Features** - Positional encoding and variational layers\n",
                "\n",
                "---\n",
                "\n",
                "## Theoretical Background\n",
                "\n",
                "### Quantum Attention Mechanism\n",
                "\n",
                "Classical attention computes similarity scores using dot products: $\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$\n",
                "\n",
                "In quantum transformers, we replace this with quantum circuits that compute overlaps between quantum states.\n",
                "\n",
                "### SWAP Test\n",
                "\n",
                "The SWAP test is a quantum algorithm for measuring the overlap (fidelity) between two quantum states $|\\psi\\rangle$ and $|\\phi\\rangle$:\n",
                "\n",
                "1. Prepare an ancilla qubit in state $|0\\rangle$\n",
                "2. Apply Hadamard to ancilla: $|+\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle + |1\\rangle)$\n",
                "3. Apply controlled-SWAP between the two states\n",
                "4. Apply Hadamard to ancilla\n",
                "5. Measure ancilla: $P(0) = \\frac{1}{2}(1 + |\\langle\\psi|\\phi\\rangle|^2)$\n",
                "\n",
                "### Quantum Feed-Forward Networks\n",
                "\n",
                "Variational quantum circuits serve as feed-forward layers with:\n",
                "- Data encoding via rotation gates (RY, RZ)\n",
                "- Parameterized rotation layers (Rot gates)\n",
                "- Entangling layers (CNOT gates)\n",
                "- Measurement to extract classical output"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Installation and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages (uncomment if needed)\n",
                "# !pip install torch>=2.0.0 pennylane>=0.32.0 matplotlib seaborn numpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Core libraries\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import pennylane as qml\n",
                "\n",
                "# Visualization\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Set style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('viridis')\n",
                "\n",
                "# Set random seeds for reproducibility\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)\n",
                "\n",
                "print(f\"Python version: {sys.version}\")\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"PennyLane version: {qml.__version__}\")\n",
                "print(f\"NumPy version: {np.__version__}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import Quantum Transformer components\n",
                "import sys\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "from quantum_transformers import (\n",
                "    QuantumTransformer,\n",
                "    QuantumTransformerConfig,\n",
                "    QuantumMultiHeadAttention,\n",
                "    QuantumAttentionConfig,\n",
                "    SwapTestAttention,\n",
                "    QuantumFeedForward,\n",
                "    QuantumPositionalEncoding,\n",
                "    QuantumSinusoidalEncoding,\n",
                "    QuantumRotationalEncoding,\n",
                "    QuantumAmplitudeEncoding,\n",
                "    QuantumAngleEncoding,\n",
                "    VariationalLayer,\n",
                "    QuantumTransformerForMolecules,\n",
                "    MolecularModelConfig,\n",
                "    SMILESTokenizer,\n",
                "    get_info,\n",
                ")\n",
                "\n",
                "# Display library info\n",
                "info = get_info()\n",
                "for key, value in info.items():\n",
                "    print(f\"{key}: {value}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Basic Quantum Transformer Experiments"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create QuantumTransformerConfig with NISQ-friendly parameters\n",
                "config = QuantumTransformerConfig(\n",
                "    n_qubits=4,       # 4 qubits for NISQ simulation\n",
                "    n_heads=2,        # 2 attention heads\n",
                "    n_layers=4,       # 4 transformer layers\n",
                "    d_model=16,       # Model dimension (must be <= 2^n_qubits * n_heads)\n",
                "    d_ff=64,          # Feed-forward dimension\n",
                "    max_seq_len=64,   # Maximum sequence length\n",
                "    dropout=0.1,      # Dropout rate\n",
                "    attention_type=\"swap_test\",\n",
                "    device=\"default.qubit\"\n",
                ")\n",
                "\n",
                "print(\"Quantum Transformer Configuration:\")\n",
                "print(f\"  - Number of qubits: {config.n_qubits}\")\n",
                "print(f\"  - Number of heads: {config.n_heads}\")\n",
                "print(f\"  - Number of layers: {config.n_layers}\")\n",
                "print(f\"  - Model dimension: {config.d_model}\")\n",
                "print(f\"  - Max sequence length: {config.max_seq_len}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create Quantum Transformer model\n",
                "model = QuantumTransformer(config)\n",
                "\n",
                "# Display model architecture\n",
                "print(\"Quantum Transformer Model:\")\n",
                "print(model)\n",
                "\n",
                "# Count parameters\n",
                "params = model.count_parameters()\n",
                "print(f\"\\nModel Parameters:\")\n",
                "print(f\"  - Total: {params['total']:,}\")\n",
                "print(f\"  - Trainable: {params['trainable']:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create dummy input tensor\n",
                "batch_size = 2\n",
                "seq_len = 8\n",
                "d_model = 16\n",
                "\n",
                "dummy_input = torch.randn(batch_size, seq_len, d_model)\n",
                "print(f\"Input shape: {dummy_input.shape}\")\n",
                "print(f\"Input sample (first 3 values of first sequence):\")\n",
                "print(dummy_input[0, 0, :3])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Forward pass through Quantum Transformer\n",
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    output = model(dummy_input)\n",
                "\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(f\"Expected shape: ({batch_size}, {seq_len}, {d_model})\")\n",
                "print(f\"\\nOutput sample (first sequence, first position):\")\n",
                "print(output[0, 0, :])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize input vs output distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "axes[0].hist(dummy_input.flatten().numpy(), bins=50, alpha=0.7, color='steelblue')\n",
                "axes[0].set_title('Input Distribution')\n",
                "axes[0].set_xlabel('Value')\n",
                "axes[0].set_ylabel('Frequency')\n",
                "\n",
                "axes[1].hist(output.flatten().numpy(), bins=50, alpha=0.7, color='darkorange')\n",
                "axes[1].set_title('Output Distribution')\n",
                "axes[1].set_xlabel('Value')\n",
                "axes[1].set_ylabel('Frequency')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Quantum Self-Attention"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create Quantum Attention Configuration\n",
                "attn_config = QuantumAttentionConfig(\n",
                "    n_qubits=4,\n",
                "    n_heads=2,\n",
                "    d_k=8,\n",
                "    attention_type=\"swap_test\",\n",
                "    dropout=0.1\n",
                ")\n",
                "\n",
                "# Create Multi-Head Attention module\n",
                "mha = QuantumMultiHeadAttention(attn_config)\n",
                "print(\"Quantum Multi-Head Attention:\")\n",
                "print(mha)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create SWAP Test Attention for visualization\n",
                "swap_attention = SwapTestAttention(n_qubits=4)\n",
                "\n",
                "# Create sample query and key tensors\n",
                "batch_size = 1\n",
                "seq_len = 4  # Small sequence for visualization\n",
                "d_k = 8\n",
                "\n",
                "query = torch.randn(batch_size, seq_len, d_k)\n",
                "key = torch.randn(batch_size, seq_len, d_k)\n",
                "\n",
                "print(f\"Query shape: {query.shape}\")\n",
                "print(f\"Key shape: {key.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute attention scores using SWAP test\n",
                "with torch.no_grad():\n",
                "    attention_scores = swap_attention(query, key)\n",
                "\n",
                "print(f\"Attention scores shape: {attention_scores.shape}\")\n",
                "print(f\"\\nAttention matrix:\")\n",
                "print(attention_scores[0].numpy())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize attention pattern as heatmap\n",
                "fig, ax = plt.subplots(figsize=(8, 6))\n",
                "\n",
                "attn_matrix = attention_scores[0].numpy()\n",
                "sns.heatmap(\n",
                "    attn_matrix,\n",
                "    annot=True,\n",
                "    fmt='.3f',\n",
                "    cmap='viridis',\n",
                "    xticklabels=[f'K{i}' for i in range(seq_len)],\n",
                "    yticklabels=[f'Q{i}' for i in range(seq_len)],\n",
                "    ax=ax\n",
                ")\n",
                "ax.set_title('Quantum Attention Pattern (SWAP Test)')\n",
                "ax.set_xlabel('Key Position')\n",
                "ax.set_ylabel('Query Position')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply softmax to get attention weights\n",
                "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Raw scores\n",
                "sns.heatmap(attention_scores[0].numpy(), annot=True, fmt='.3f',\n",
                "            cmap='coolwarm', ax=axes[0])\n",
                "axes[0].set_title('Raw Attention Scores')\n",
                "axes[0].set_xlabel('Key Position')\n",
                "axes[0].set_ylabel('Query Position')\n",
                "\n",
                "# Softmax weights\n",
                "sns.heatmap(attention_weights[0].numpy(), annot=True, fmt='.3f',\n",
                "            cmap='viridis', ax=axes[1])\n",
                "axes[1].set_title('Attention Weights (After Softmax)')\n",
                "axes[1].set_xlabel('Key Position')\n",
                "axes[1].set_ylabel('Query Position')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Molecular Property Prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize SMILES Tokenizer\n",
                "tokenizer = SMILESTokenizer()\n",
                "\n",
                "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
                "print(f\"PAD token ID: {tokenizer.pad_token_id}\")\n",
                "print(f\"UNK token ID: {tokenizer.unk_token_id}\")\n",
                "print(f\"\\nSample vocabulary entries:\")\n",
                "for token, idx in list(tokenizer.vocab.items())[:10]:\n",
                "    print(f\"  '{token}': {idx}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample molecules for prediction\n",
                "molecules = {\n",
                "    \"Ethanol\": \"CCO\",\n",
                "    \"Cyclohexane\": \"C1CCCCC1\",\n",
                "    \"Carbon Dioxide\": \"O=C=O\",\n",
                "    \"Benzene\": \"c1ccccc1\",\n",
                "    \"Methane\": \"C\",\n",
                "}\n",
                "\n",
                "# Tokenize molecules\n",
                "print(\"Tokenization Results:\")\n",
                "print(\"-\" * 60)\n",
                "for name, smiles in molecules.items():\n",
                "    tokens = tokenizer.tokenize(smiles)\n",
                "    encoded = tokenizer.encode(smiles, max_length=32)\n",
                "    print(f\"{name} ({smiles}):\")\n",
                "    print(f\"  Tokens: {tokens}\")\n",
                "    print(f\"  Encoded: {encoded[:10]}...\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create Molecular Model Configuration\n",
                "mol_config = MolecularModelConfig(\n",
                "    vocab_size=tokenizer.vocab_size,\n",
                "    max_seq_len=32,\n",
                "    n_qubits=4,\n",
                "    n_heads=2,\n",
                "    n_layers=2,  # Reduced for faster execution\n",
                "    d_model=16,\n",
                "    dropout=0.1,\n",
                "    task=\"energy\"\n",
                ")\n",
                "\n",
                "# Create model\n",
                "mol_model = QuantumTransformerForMolecules(mol_config)\n",
                "print(\"Molecular Transformer Configuration:\")\n",
                "print(f\"  - Task: {mol_config.task}\")\n",
                "print(f\"  - Vocab size: {mol_config.vocab_size}\")\n",
                "print(f\"  - Model dimension: {mol_config.d_model}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict energies for molecules\n",
                "mol_model.eval()\n",
                "predictions = {}\n",
                "\n",
                "print(\"Energy Predictions:\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "for name, smiles in molecules.items():\n",
                "    input_ids = tokenizer(smiles, max_length=32, return_tensors=\"pt\")\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        energy = mol_model.predict_energy(input_ids)\n",
                "    \n",
                "    predictions[name] = energy.item()\n",
                "    print(f\"{name}: {energy.item():.4f} Ha\")\n",
                "\n",
                "print(\"\\n(Note: Values are from untrained model for demonstration)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize predictions\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "\n",
                "names = list(predictions.keys())\n",
                "values = list(predictions.values())\n",
                "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(names)))\n",
                "\n",
                "bars = ax.bar(names, values, color=colors, edgecolor='black', linewidth=1.2)\n",
                "ax.set_ylabel('Predicted Energy (Hartree)')\n",
                "ax.set_title('Quantum Transformer Molecular Energy Predictions')\n",
                "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
                "\n",
                "# Add value labels\n",
                "for bar, val in zip(bars, values):\n",
                "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
                "            f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
                "\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Benchmarking and Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_mae(predictions, targets):\n",
                "    \"\"\"Calculate Mean Absolute Error.\"\"\"\n",
                "    predictions = np.array(predictions)\n",
                "    targets = np.array(targets)\n",
                "    return np.mean(np.abs(predictions - targets))\n",
                "\n",
                "def calculate_rmse(predictions, targets):\n",
                "    \"\"\"Calculate Root Mean Square Error.\"\"\"\n",
                "    predictions = np.array(predictions)\n",
                "    targets = np.array(targets)\n",
                "    return np.sqrt(np.mean((predictions - targets) ** 2))\n",
                "\n",
                "# Reference energies (dummy data for demonstration)\n",
                "reference_energies = {\n",
                "    \"Ethanol\": -154.08,\n",
                "    \"Cyclohexane\": -234.51,\n",
                "    \"Carbon Dioxide\": -188.22,\n",
                "    \"Benzene\": -232.15,\n",
                "    \"Methane\": -40.42,\n",
                "}\n",
                "\n",
                "print(\"Reference vs Predicted Energies:\")\n",
                "print(\"-\" * 50)\n",
                "for name in molecules.keys():\n",
                "    ref = reference_energies[name]\n",
                "    pred = predictions[name]\n",
                "    error = abs(ref - pred)\n",
                "    print(f\"{name}:\")\n",
                "    print(f\"  Reference: {ref:.2f} Ha\")\n",
                "    print(f\"  Predicted: {pred:.4f} Ha\")\n",
                "    print(f\"  |Error|: {error:.2f} Ha\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Classical Transformer for comparison (simplified)\n",
                "class ClassicalTransformer(nn.Module):\n",
                "    def __init__(self, vocab_size, d_model=16, n_heads=2, n_layers=2):\n",
                "        super().__init__()\n",
                "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
                "        encoder_layer = nn.TransformerEncoderLayer(\n",
                "            d_model=d_model, nhead=n_heads, batch_first=True\n",
                "        )\n",
                "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
                "        self.head = nn.Linear(d_model, 1)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.embedding(x)\n",
                "        x = self.encoder(x)\n",
                "        x = x.mean(dim=1)\n",
                "        return self.head(x)\n",
                "\n",
                "# Create classical model\n",
                "classical_model = ClassicalTransformer(tokenizer.vocab_size)\n",
                "classical_model.eval()\n",
                "\n",
                "# Get classical predictions\n",
                "classical_predictions = {}\n",
                "for name, smiles in molecules.items():\n",
                "    input_ids = tokenizer(smiles, max_length=32, return_tensors=\"pt\")\n",
                "    with torch.no_grad():\n",
                "        energy = classical_model(input_ids)\n",
                "    classical_predictions[name] = energy.item()\n",
                "\n",
                "print(\"Classical Transformer Predictions:\")\n",
                "for name, val in classical_predictions.items():\n",
                "    print(f\"  {name}: {val:.4f} Ha\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate MAE for both models\n",
                "ref_values = list(reference_energies.values())\n",
                "quantum_preds = list(predictions.values())\n",
                "classical_preds = list(classical_predictions.values())\n",
                "\n",
                "quantum_mae = calculate_mae(quantum_preds, ref_values)\n",
                "classical_mae = calculate_mae(classical_preds, ref_values)\n",
                "\n",
                "quantum_rmse = calculate_rmse(quantum_preds, ref_values)\n",
                "classical_rmse = calculate_rmse(classical_preds, ref_values)\n",
                "\n",
                "print(\"Performance Comparison:\")\n",
                "print(\"-\" * 40)\n",
                "print(f\"Quantum Transformer MAE:  {quantum_mae:.2f} Ha\")\n",
                "print(f\"Classical Transformer MAE: {classical_mae:.2f} Ha\")\n",
                "print(f\"Quantum Transformer RMSE:  {quantum_rmse:.2f} Ha\")\n",
                "print(f\"Classical Transformer RMSE: {classical_rmse:.2f} Ha\")\n",
                "print(\"\\n(Note: Both models are untrained - metrics for comparison only)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize performance comparison\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# MAE Comparison\n",
                "models = ['Quantum Transformer', 'Classical Transformer']\n",
                "mae_values = [quantum_mae, classical_mae]\n",
                "colors = ['#2ecc71', '#e74c3c']\n",
                "\n",
                "axes[0].bar(models, mae_values, color=colors, edgecolor='black', linewidth=1.5)\n",
                "axes[0].set_ylabel('Mean Absolute Error (Ha)')\n",
                "axes[0].set_title('MAE Comparison')\n",
                "for i, v in enumerate(mae_values):\n",
                "    axes[0].text(i, v + 1, f'{v:.2f}', ha='center', va='bottom', fontweight='bold')\n",
                "\n",
                "# Prediction Comparison\n",
                "x = np.arange(len(molecules))\n",
                "width = 0.25\n",
                "\n",
                "axes[1].bar(x - width, ref_values, width, label='Reference', color='#3498db')\n",
                "axes[1].bar(x, quantum_preds, width, label='Quantum', color='#2ecc71')\n",
                "axes[1].bar(x + width, classical_preds, width, label='Classical', color='#e74c3c')\n",
                "\n",
                "axes[1].set_ylabel('Energy (Ha)')\n",
                "axes[1].set_title('Prediction Comparison by Molecule')\n",
                "axes[1].set_xticks(x)\n",
                "axes[1].set_xticklabels(list(molecules.keys()), rotation=45, ha='right')\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Advanced Quantum Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quantum Positional Encoding\n",
                "d_model = 16\n",
                "max_len = 32\n",
                "n_qubits = 4\n",
                "\n",
                "pos_encoder = QuantumSinusoidalEncoding(d_model=d_model, max_len=max_len, n_qubits=n_qubits)\n",
                "\n",
                "# Get positional encodings\n",
                "pe = pos_encoder.pe.numpy()\n",
                "\n",
                "print(f\"Positional Encoding shape: {pe.shape}\")\n",
                "print(f\"Sample encoding for position 0: {pe[0, :4]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize positional encodings\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Heatmap of positional encodings\n",
                "sns.heatmap(pe[:16, :], cmap='RdBu_r', center=0, ax=axes[0])\n",
                "axes[0].set_title('Positional Encoding Heatmap')\n",
                "axes[0].set_xlabel('Dimension')\n",
                "axes[0].set_ylabel('Position')\n",
                "\n",
                "# Line plot of first 4 dimensions\n",
                "positions = np.arange(max_len)\n",
                "for dim in range(4):\n",
                "    axes[1].plot(positions, pe[:, dim], label=f'Dim {dim}', linewidth=2)\n",
                "axes[1].set_title('Positional Encoding Curves')\n",
                "axes[1].set_xlabel('Position')\n",
                "axes[1].set_ylabel('Encoding Value')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quantum Amplitude Encoding\n",
                "amplitude_encoder = QuantumAmplitudeEncoding(n_qubits=4)\n",
                "\n",
                "# Create sample input\n",
                "sample_input = torch.randn(4, 16)  # 4 samples, 16 features\n",
                "\n",
                "# Before encoding - visualize input amplitudes\n",
                "print(\"Before Encoding:\")\n",
                "print(f\"  Input shape: {sample_input.shape}\")\n",
                "print(f\"  Input range: [{sample_input.min():.3f}, {sample_input.max():.3f}]\")\n",
                "\n",
                "# After encoding\n",
                "encoded = amplitude_encoder(sample_input)\n",
                "print(f\"\\nAfter Encoding:\")\n",
                "print(f\"  Encoded shape: {encoded.shape}\")\n",
                "print(f\"  Encoded norm: {torch.norm(encoded[0]):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize quantum amplitudes before and after encoding\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Before encoding (normalized input)\n",
                "normalized_input = sample_input / (sample_input.norm(dim=-1, keepdim=True) + 1e-8)\n",
                "axes[0].bar(range(16), normalized_input[0].numpy(), color='steelblue', alpha=0.7)\n",
                "axes[0].set_title('Normalized Input Amplitudes')\n",
                "axes[0].set_xlabel('Feature Index')\n",
                "axes[0].set_ylabel('Amplitude')\n",
                "\n",
                "# After encoding (quantum state amplitudes)\n",
                "if encoded.shape[-1] == 16:\n",
                "    real_parts = encoded[0].real.numpy() if encoded.is_complex() else encoded[0].numpy()\n",
                "    axes[1].bar(range(len(real_parts)), np.abs(real_parts), color='darkorange', alpha=0.7)\n",
                "else:\n",
                "    axes[1].bar(range(encoded.shape[-1]), np.abs(encoded[0].numpy()), color='darkorange', alpha=0.7)\n",
                "axes[1].set_title('Quantum State Amplitudes')\n",
                "axes[1].set_xlabel('Basis State Index')\n",
                "axes[1].set_ylabel('|Amplitude|')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Variational Feed-Forward Layer\n",
                "ffn = QuantumFeedForward(\n",
                "    d_model=16,\n",
                "    n_qubits=4,\n",
                "    n_layers=3,\n",
                "    dropout=0.0,\n",
                "    entanglement=\"circular\"\n",
                ")\n",
                "\n",
                "# Create batch input\n",
                "batch_input = torch.randn(2, 4, 16)  # batch=2, seq=4, d_model=16\n",
                "\n",
                "print(\"Variational Feed-Forward Layer:\")\n",
                "print(f\"  Input shape: {batch_input.shape}\")\n",
                "\n",
                "# Forward pass\n",
                "with torch.no_grad():\n",
                "    ffn_output = ffn(batch_input)\n",
                "\n",
                "print(f\"  Output shape: {ffn_output.shape}\")\n",
                "print(f\"  \\nWeight parameters:\")\n",
                "print(f\"    Shape: {ffn.weights.shape}\")\n",
                "print(f\"    Range: [{ffn.weights.min():.3f}, {ffn.weights.max():.3f}]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize FFN transformation\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "# Input distribution\n",
                "axes[0].hist(batch_input.flatten().numpy(), bins=30, color='steelblue', alpha=0.7)\n",
                "axes[0].set_title('Input Distribution')\n",
                "axes[0].set_xlabel('Value')\n",
                "\n",
                "# Output distribution\n",
                "axes[1].hist(ffn_output.flatten().numpy(), bins=30, color='darkorange', alpha=0.7)\n",
                "axes[1].set_title('Output Distribution (After Variational Circuit)')\n",
                "axes[1].set_xlabel('Value')\n",
                "\n",
                "# Transformation visualization\n",
                "in_flat = batch_input[0, 0, :8].numpy()\n",
                "out_flat = ffn_output[0, 0, :8].numpy()\n",
                "x = np.arange(8)\n",
                "width = 0.35\n",
                "\n",
                "axes[2].bar(x - width/2, in_flat, width, label='Input', color='steelblue')\n",
                "axes[2].bar(x + width/2, out_flat, width, label='Output', color='darkorange')\n",
                "axes[2].set_title('Input vs Output (First 8 dims)')\n",
                "axes[2].set_xlabel('Dimension')\n",
                "axes[2].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize variational circuit weights\n",
                "weights = ffn.weights.detach().numpy()\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "gate_names = ['RX', 'RY', 'RZ']\n",
                "for layer_idx in range(3):\n",
                "    im = axes[layer_idx].imshow(weights[layer_idx], cmap='coolwarm', aspect='auto')\n",
                "    axes[layer_idx].set_title(f'Layer {layer_idx + 1} Rotation Parameters')\n",
                "    axes[layer_idx].set_xlabel('Rotation Gate')\n",
                "    axes[layer_idx].set_ylabel('Qubit')\n",
                "    axes[layer_idx].set_xticks([0, 1, 2])\n",
                "    axes[layer_idx].set_xticklabels(gate_names)\n",
                "    plt.colorbar(im, ax=axes[layer_idx])\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "This notebook demonstrated the key features of the Quantum Transformer library:\n",
                "\n",
                "1. **Basic Model Creation**: Built a QuantumTransformer with 4 qubits, 2 heads, and 4 layers\n",
                "2. **SWAP Test Attention**: Visualized quantum attention patterns using the SWAP test algorithm\n",
                "3. **Molecular Prediction**: Used SMILES tokenization and energy prediction for molecules\n",
                "4. **Benchmarking**: Compared Quantum vs Classical Transformer performance\n",
                "5. **Advanced Features**: Explored positional encoding, amplitude encoding, and variational layers\n",
                "\n",
                "### Key Takeaways\n",
                "\n",
                "- Quantum Transformers use quantum circuits for attention computation\n",
                "- SWAP test measures state overlap for similarity scoring\n",
                "- Variational circuits provide trainable non-linear transformations\n",
                "- NISQ-friendly designs use 4-8 qubits for practical simulation\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "- Train models on real QM9 dataset for molecular property prediction\n",
                "- Experiment with different entanglement patterns\n",
                "- Deploy on real quantum hardware via PennyLane plugins"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}