{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#  Quantum Transformer Fundamentals\n",
                "\n",
                "**Quantum Transformer Tutorial 01**\n",
                "\n",
                "This notebook introduces the Quantum Transformer architecture where ALL attention and feed-forward computations are performed using quantum circuits.\n",
                "\n",
                "## What You'll Learn\n",
                "\n",
                "1. Quantum attention via SWAP test\n",
                "2. Quantum positional encoding\n",
                "3. Quantum feed-forward networks\n",
                "4. Building a complete Quantum Transformer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "import pennylane as qml\n",
                "\n",
                "from quantum_transformers import (\n",
                "    QuantumTransformer,\n",
                "    QuantumTransformerConfig,\n",
                "    get_info,\n",
                ")\n",
                "\n",
                "print(get_info())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Quantum Attention Mechanism\n",
                "\n",
                "Unlike classical attention, quantum attention computes similarity using **SWAP test**:\n",
                "\n",
                "```\n",
                "|0\u27e9 \u2500\u2500H\u2500\u2500\u25cf\u2500\u2500H\u2500\u2500 Measure  \u2192 P(0) = (1 + |\u27e8\u03c8|\u03c6\u27e9|\u00b2)/2\n",
                "         \u2502\n",
                "|\u03c8\u27e9 \u2500\u2500\u2500\u2500\u2500X\u2500\u2500\u2500\u2500\u2500 (Query)\n",
                "         \u2502\n",
                "|\u03c6\u27e9 \u2500\u2500\u2500\u2500\u2500X\u2500\u2500\u2500\u2500\u2500 (Key)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from quantum_transformers.circuits import SwapTestCircuit\n",
                "\n",
                "# Create SWAP test circuit\n",
                "swap_test = SwapTestCircuit(n_qubits=2)\n",
                "\n",
                "# Test with similar states\n",
                "query = torch.tensor([0.5, 0.5])\n",
                "key = torch.tensor([0.5, 0.6])\n",
                "\n",
                "similarity = swap_test(query, key)\n",
                "print(f\"Quantum similarity (SWAP test): {similarity.item():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Building a Quantum Transformer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configure Quantum Transformer\n",
                "config = QuantumTransformerConfig(\n",
                "    n_qubits=2,\n",
                "    n_heads=2,\n",
                "    n_layers=2,\n",
                "    d_model=16,\n",
                "    max_seq_len=32,\n",
                ")\n",
                "\n",
                "# Create model\n",
                "model = QuantumTransformer(config)\n",
                "print(f\"Parameters: {model.count_parameters()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Forward pass\n",
                "x = torch.randn(2, 8, 16)  # batch=2, seq=8, d=16\n",
                "output = model(x)\n",
                "\n",
                "print(f\"Input: {x.shape}\")\n",
                "print(f\"Output: {output.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Quantum Feed-Forward Network\n",
                "\n",
                "Replaces classical FFN with variational quantum circuits."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from quantum_transformers.layers import QuantumFeedForward\n",
                "\n",
                "qffn = QuantumFeedForward(d_model=16, n_qubits=2, n_layers=2)\n",
                "\n",
                "x = torch.randn(4, 8, 16)\n",
                "output = qffn(x)\n",
                "\n",
                "print(f\"Quantum FFN output: {output.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "- Quantum Transformer uses quantum circuits for ALL computations\n",
                "- SWAP test computes attention similarity\n",
                "- Variational circuits implement feed-forward layers\n",
                "- End-to-end differentiable via parameter-shift rule\n",
                "\n",
                "**Next**: [02_molecular_prediction.ipynb](02_molecular_prediction.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}