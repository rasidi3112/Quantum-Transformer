{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#  Molecular Property Prediction with Quantum Transformer\n",
                "\n",
                "**Quantum Transformer Tutorial 02**\n",
                "\n",
                "Apply the Quantum Transformer to predict molecular properties from SMILES strings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from quantum_transformers.molecular import (\n",
                "    QuantumTransformerForMolecules,\n",
                "    MolecularEnergyPredictor,\n",
                "    SMILESTokenizer,\n",
                "    MolecularModelConfig,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Tokenizing Molecules"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tokenizer = SMILESTokenizer()\n",
                "\n",
                "molecules = [\n",
                "    \"CCO\",      # Ethanol\n",
                "    \"CC(=O)O\",  # Acetic acid\n",
                "    \"c1ccccc1\", # Benzene\n",
                "]\n",
                "\n",
                "for mol in molecules:\n",
                "    tokens = tokenizer.tokenize(mol)\n",
                "    print(f\"{mol:12s} \u2192 {tokens}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Create Molecular Quantum Transformer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "config = MolecularModelConfig(\n",
                "    vocab_size=tokenizer.vocab_size,\n",
                "    n_qubits=2,\n",
                "    n_heads=2,\n",
                "    n_layers=2,\n",
                "    d_model=16,\n",
                "    task=\"energy\",\n",
                ")\n",
                "\n",
                "model = QuantumTransformerForMolecules(config)\n",
                "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict energies\n",
                "tokens = tokenizer(molecules, max_length=16, return_tensors=\"pt\")\n",
                "energies = model(tokens)\n",
                "\n",
                "for mol, energy in zip(molecules, energies):\n",
                "    print(f\"{mol:12s} \u2192 Energy: {energy.item():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from quantum_transformers.optimization import QuantumAdamOptimizer\n",
                "\n",
                "# Synthetic training data\n",
                "train_smiles = [\"C\", \"CC\", \"CCC\", \"CCO\", \"CCCO\", \"c1ccccc1\"]\n",
                "train_energies = torch.tensor([[-1.0], [-2.0], [-3.0], [-2.5], [-3.5], [-5.0]])\n",
                "\n",
                "train_tokens = tokenizer(train_smiles, max_length=16, return_tensors=\"pt\")\n",
                "\n",
                "optimizer = QuantumAdamOptimizer(model.parameters(), lr=0.01)\n",
                "criterion = torch.nn.MSELoss()\n",
                "\n",
                "# Training loop\n",
                "for epoch in range(20):\n",
                "    optimizer.zero_grad()\n",
                "    pred = model(train_tokens)\n",
                "    loss = criterion(pred, train_energies)\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    \n",
                "    if epoch % 5 == 0:\n",
                "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "- SMILESTokenizer converts molecules to tokens\n",
                "- QuantumTransformerForMolecules predicts properties\n",
                "- Training uses quantum-aware optimization\n",
                "\n",
                "**Next**: [03_quantum_attention_analysis.ipynb](03_quantum_attention_analysis.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}